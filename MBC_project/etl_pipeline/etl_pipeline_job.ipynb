{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2f5b0443-457b-4a88-ba00-17fc05c879c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, explode, to_timestamp, concat_ws, when, row_number, unix_timestamp, date_format, lag, broadcast\n",
    "from pyspark.sql.window import Window\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b983af22-0332-4c39-a33b-4511842d8e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "def create_spark_session():\n",
    "    spark = SparkSession.builder\\\n",
    "        .appName(\"ETL_Job\")\\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.autoBroadcastJoinThreshold\", \"20MB\")\\\n",
    "        .config(\"spark.sql.dynamicPartitionPruning.enabled\", \"true\")\\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "# Enable Adaptive Query Execution  # Optimize partitions  # Optimize shuffle  # Enable broadcast joins for small tables\n",
    "# Enable dynamic partition pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5b257ef2-d360-4748-ab42-3c7ebf8b3a56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "395fa9b9-a183-4a97-a406-d182c49794c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_logs_folder = \"C:/Users/vrnp2/MBC\"  # Folder containing JSON files for raw logs\n",
    "program_logs_file = \"C:/Users/vrnp2/Mbc_program_file/program_data.txt\"  # Path to program_logs file (single file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "bd60b305-c532-46d4-b146-120d1563718f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the folder:\n",
      "C:/Users/vrnp2/MBC\\events_20240101.json\n",
      "C:/Users/vrnp2/MBC\\events_20240102.json\n",
      "C:/Users/vrnp2/MBC\\events_20240103.json\n",
      "C:/Users/vrnp2/MBC\\events_20240104.json\n",
      "C:/Users/vrnp2/MBC\\events_20240105.json\n",
      "C:/Users/vrnp2/MBC\\events_20240106.json\n",
      "C:/Users/vrnp2/MBC\\events_20240107.json\n"
     ]
    }
   ],
   "source": [
    "# List files in the raw logs folder\n",
    "\n",
    "import os\n",
    "raw_logs_files = [os.path.join(raw_logs_folder, f) for f in os.listdir(raw_logs_folder) if f.endswith(\".json\")]\n",
    "\n",
    "print(\"Files in the folder:\")\n",
    "for file in raw_logs_files:\n",
    "    print(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b05e7761-125e-492c-8cc0-15276f7689aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------------+\n",
      "|program_id|program_name|program_genre|\n",
      "+----------+------------+-------------+\n",
      "|       158| Program 158|       Comedy|\n",
      "|       168| Program 168|       Comedy|\n",
      "+----------+------------+-------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "Program logs loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col, split,explode\n",
    "\n",
    "# Define schema for raw logs\n",
    "raw_logs_schema = StructType([\n",
    "    StructField(\"mac_id\", StringType(), True),\n",
    "    StructField(\"event_date\", StringType(), True),\n",
    "    StructField(\"event_time\", StringType(), True),\n",
    "    StructField(\"channel_name\", StringType(), True),\n",
    "    StructField(\"program_id\", StringType(), True),\n",
    "    StructField(\"geo_location\", StringType(), True),\n",
    "    StructField(\"event_code\", StringType(), True),\n",
    "    StructField(\"satellite_name\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define schema for program logs\n",
    "program_logs_schema = StructType([\n",
    "    StructField(\"program_id\", StringType(), True),\n",
    "    StructField(\"program_name\", StringType(), True),\n",
    "    StructField(\"program_genre\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Read the text file\n",
    "program_logs_df_raw = spark.read.text(program_logs_file)\n",
    "\n",
    "# Split the rows into columns based on the tab delimiter\n",
    "program_logs_df_split = program_logs_df_raw.select(\n",
    "    split(col(\"value\"), \"\\t\")[0].alias(\"program_id\"),\n",
    "    split(col(\"value\"), \"\\t\")[1].alias(\"program_name\"),\n",
    "    split(col(\"value\"), \"\\t\")[2].alias(\"program_genre\")\n",
    ")\n",
    "\n",
    "# Filter out the header row\n",
    "program_logs_df = program_logs_df_split.filter(col(\"program_id\") != \"program_id\")\n",
    "\n",
    "\n",
    "program_logs_df.show(2)\n",
    "print(\"Program logs loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "98c2a498-6cfd-4ba7-ac28-ab15a4c477a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_transform_json(raw_logs_df, program_logs_df):\n",
    "    # exploded_df = raw_logs_df.select(explode(col(\"events_20240101\")).alias(\"event\"))\n",
    "    exploded_df = raw_logs_df.select(explode(col(dynamic_key)).alias(\"event\"))\n",
    "\n",
    "    # Extract fields from the exploded data\n",
    "    transformed_df = exploded_df.select(\n",
    "        col(\"event.mac\").alias(\"mac_id\"),\n",
    "        col(\"event.eventdate\").alias(\"event_date\"),\n",
    "        col(\"event.eventtime\").alias(\"event_time\"),\n",
    "        col(\"event.chname\").alias(\"channel_name\"),\n",
    "        col(\"event.program_id\").alias(\"program_id\"),\n",
    "        col(\"event.geo_location\").alias(\"geo_location\"),\n",
    "        col(\"event.code\").alias(\"event_code\"),\n",
    "        col(\"event.sat\").alias(\"satellite_name\"),\n",
    "        col(\"event.ts\").alias(\"timestamp\"),\n",
    "        col(\"event.indextime\").alias(\"index_time\")\n",
    "    )\n",
    "\n",
    "    # handling null records \n",
    "    transformed_df = transformed_df.fillna({\"geo_location\": \"Unknown\", \"index_time\": \"N/A\"}) \n",
    "\n",
    "    # Combine event_date and event_time into a single datetime column\n",
    "    transformed_df = transformed_df.withColumn(\n",
    "        \"datetime\",\n",
    "        to_timestamp(concat_ws(\" \", col(\"event_date\"), col(\"event_time\")), \"yyyy-MM-dd HH:mm:ss\")\n",
    "    ).drop(\"event_date\", \"event_time\")\n",
    "\n",
    "    # Extract date from the datetime column\n",
    "    raw_logs_df = raw_logs_df.withColumn(\"date\", date_format(col(\"datetime\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "    enriched_df = enriched_df.repartition(\"date\")   \n",
    "    # repartitioniing based on the date column to increase parellellism\n",
    "\n",
    "\n",
    "    # Join with program metadata for enrichment\n",
    "    enriched_df = transformed_df.join(\n",
    "        broadcast(program_logs_df),\n",
    "        transformed_df.program_id == program_logs_df.program_id,\n",
    "        \"left\"\n",
    "    ).drop(program_logs_df.program_id)\n",
    "\n",
    "    # Categorize users based on geo_location and program_genre\n",
    "    enriched_df = enriched_df.withColumn(\n",
    "        \"user_category\",\n",
    "        when(col(\"geo_location\").isNotNull(), \"Known Location\").otherwise(\"Unknown Location\")\n",
    "    ).withColumn(\n",
    "        \"program_category\",\n",
    "        when(col(\"program_genre\").like(\"%Comedy%\"), \"Comedy Viewer\")\n",
    "        .when(col(\"program_genre\").like(\"%News%\"), \"News Viewer\")\n",
    "        .otherwise(\"General Viewer\")\n",
    "    )\n",
    "\n",
    "    # Deduplicate data \n",
    "    window_spec = Window.partitionBy(\"mac_id\", \"datetime\").orderBy(col(\"datetime\").desc())\n",
    "    deduped_df = enriched_df.withColumn(\"row_num\", row_number().over(window_spec)).filter(col(\"row_num\") == 1).drop(\"row_num\")\n",
    "\n",
    "        # Define window specification to calculate duration for each user\n",
    "    window_spec = Window.partitionBy(\"mac_id\").orderBy(\"datetime\")\n",
    "    \n",
    "    # Calculate duration as the difference between consecutive events\n",
    "    deduped_df = deduped_df.withColumn(\n",
    "        \"duration_in_seconds\",\n",
    "        unix_timestamp(lag(\"datetime\").over(window_spec)) - unix_timestamp(\"datetime\")\n",
    "    ).fillna(0, subset=[\"duration_in_seconds\"])  # Fill null durations with 0\n",
    "\n",
    "    \n",
    "\n",
    "    return deduped_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c80800b9-6952-40a4-b849-29e75135713f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:/Users/vrnp2/MBC\\events_20240101.json\n",
      "Dynamic key found in the JSON file: events_20240101\n",
      "Raw Data Table:\n",
      "+--------------------+-------------------+--------------+----------+------------------+----------+--------------+------------------+--------------------+-------------------+\n",
      "|              mac_id|           datetime|  channel_name|program_id|      geo_location|event_code|satellite_name|         timestamp|          index_time|duration_in_seconds|\n",
      "+--------------------+-------------------+--------------+----------+------------------+----------+--------------+------------------+--------------------+-------------------+\n",
      "|4136b85db09a646f2...|2024-01-01 00:09:22|SSC Extra 1 HD|         0|[21.0362, 52.2394]|    SALIVE|      BADR_4&6|1709076809.0243657|2024-01-01T00:37:...|                  0|\n",
      "+--------------------+-------------------+--------------+----------+------------------+----------+--------------+------------------+--------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Downstream Table:\n",
      "+--------------------+-------------------+--------------+----------+\n",
      "|              mac_id|           datetime|  channel_name|program_id|\n",
      "+--------------------+-------------------+--------------+----------+\n",
      "|4136b85db09a646f2...|2024-01-01 00:09:00|SSC Extra 1 HD|         0|\n",
      "+--------------------+-------------------+--------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Error loading data into Redshift for file C:/Users/vrnp2/MBC\\events_20240101.json: name 'redshift_url' is not defined\n",
      "Processing file: C:/Users/vrnp2/MBC\\events_20240102.json\n",
      "Dynamic key found in the JSON file: events_20240102\n",
      "Raw Data Table:\n",
      "+--------------------+-------------------+--------------+----------+------------------+----------+--------------+------------------+--------------------+-------------------+\n",
      "|              mac_id|           datetime|  channel_name|program_id|      geo_location|event_code|satellite_name|         timestamp|          index_time|duration_in_seconds|\n",
      "+--------------------+-------------------+--------------+----------+------------------+----------+--------------+------------------+--------------------+-------------------+\n",
      "|4136b85db09a646f2...|2024-01-02 00:09:22|SSC Extra 1 HD|         0|[21.0362, 52.2394]|    SALIVE|      BADR_4&6|1709076809.0243657|2024-01-02T00:10:...|                  0|\n",
      "+--------------------+-------------------+--------------+----------+------------------+----------+--------------+------------------+--------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Downstream Table:\n",
      "+--------------------+-------------------+--------------+----------+\n",
      "|              mac_id|           datetime|  channel_name|program_id|\n",
      "+--------------------+-------------------+--------------+----------+\n",
      "|4136b85db09a646f2...|2024-01-02 00:09:00|SSC Extra 1 HD|         0|\n",
      "+--------------------+-------------------+--------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Error loading data into Redshift for file C:/Users/vrnp2/MBC\\events_20240102.json: name 'redshift_url' is not defined\n",
      "Processing file: C:/Users/vrnp2/MBC\\events_20240103.json\n",
      "Dynamic key found in the JSON file: events_20240103\n",
      "Raw Data Table:\n",
      "+--------------------+-------------------+--------------+----------+------------------+----------+--------------+------------------+--------------------+-------------------+\n",
      "|              mac_id|           datetime|  channel_name|program_id|      geo_location|event_code|satellite_name|         timestamp|          index_time|duration_in_seconds|\n",
      "+--------------------+-------------------+--------------+----------+------------------+----------+--------------+------------------+--------------------+-------------------+\n",
      "|4136b85db09a646f2...|2024-01-03 00:09:22|SSC Extra 1 HD|         0|[21.0362, 52.2394]|    SALIVE|      BADR_4&6|1709078990.1217587|2024-01-03T01:22:...|                  0|\n",
      "+--------------------+-------------------+--------------+----------+------------------+----------+--------------+------------------+--------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Downstream Table:\n",
      "+--------------------+-------------------+--------------+----------+\n",
      "|              mac_id|           datetime|  channel_name|program_id|\n",
      "+--------------------+-------------------+--------------+----------+\n",
      "|4136b85db09a646f2...|2024-01-03 00:09:00|SSC Extra 1 HD|         0|\n",
      "+--------------------+-------------------+--------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Error loading data into Redshift for file C:/Users/vrnp2/MBC\\events_20240103.json: name 'redshift_url' is not defined\n",
      "Processing file: C:/Users/vrnp2/MBC\\events_20240104.json\n",
      "Dynamic key found in the JSON file: events_20240104\n",
      "Raw Data Table:\n",
      "+--------------------+-------------------+--------------+----------+------------------+----------+--------------+-----------------+--------------------+-------------------+\n",
      "|              mac_id|           datetime|  channel_name|program_id|      geo_location|event_code|satellite_name|        timestamp|          index_time|duration_in_seconds|\n",
      "+--------------------+-------------------+--------------+----------+------------------+----------+--------------+-----------------+--------------------+-------------------+\n",
      "|4136b85db09a646f2...|2024-01-04 00:09:22|SSC Extra 1 HD|         0|[21.0362, 52.2394]|    SALIVE|      BADR_4&6|1709080108.232373|2024-01-04T15:23:...|                  0|\n",
      "+--------------------+-------------------+--------------+----------+------------------+----------+--------------+-----------------+--------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Downstream Table:\n",
      "+--------------------+-------------------+--------------+----------+\n",
      "|              mac_id|           datetime|  channel_name|program_id|\n",
      "+--------------------+-------------------+--------------+----------+\n",
      "|4136b85db09a646f2...|2024-01-04 00:09:00|SSC Extra 1 HD|         0|\n",
      "+--------------------+-------------------+--------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Error loading data into Redshift for file C:/Users/vrnp2/MBC\\events_20240104.json: name 'redshift_url' is not defined\n",
      "Processing file: C:/Users/vrnp2/MBC\\events_20240105.json\n",
      "Dynamic key found in the JSON file: events_20240105\n",
      "Raw Data Table:\n",
      "+--------------------+-------------------+--------------+----------+------------------+----------+--------------+------------------+--------------------+-------------------+\n",
      "|              mac_id|           datetime|  channel_name|program_id|      geo_location|event_code|satellite_name|         timestamp|          index_time|duration_in_seconds|\n",
      "+--------------------+-------------------+--------------+----------+------------------+----------+--------------+------------------+--------------------+-------------------+\n",
      "|4136b85db09a646f2...|2024-01-05 00:09:31|SSC Extra 1 HD|         0|[21.0362, 52.2394]|    SALIVE|      BADR_4&6|1709081252.5280542|2024-01-05T01:34:...|                  0|\n",
      "+--------------------+-------------------+--------------+----------+------------------+----------+--------------+------------------+--------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Downstream Table:\n",
      "+--------------------+-------------------+--------------+----------+\n",
      "|              mac_id|           datetime|  channel_name|program_id|\n",
      "+--------------------+-------------------+--------------+----------+\n",
      "|4136b85db09a646f2...|2024-01-05 00:09:00|SSC Extra 1 HD|         0|\n",
      "+--------------------+-------------------+--------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Error loading data into Redshift for file C:/Users/vrnp2/MBC\\events_20240105.json: name 'redshift_url' is not defined\n",
      "Processing file: C:/Users/vrnp2/MBC\\events_20240106.json\n",
      "Dynamic key found in the JSON file: events_20240106\n",
      "Raw Data Table:\n",
      "+--------------------+-------------------+------------+----------+------------------+----------+--------------+------------------+--------------------+-------------------+\n",
      "|              mac_id|           datetime|channel_name|program_id|      geo_location|event_code|satellite_name|         timestamp|          index_time|duration_in_seconds|\n",
      "+--------------------+-------------------+------------+----------+------------------+----------+--------------+------------------+--------------------+-------------------+\n",
      "|4136b85db09a646f2...|2024-01-06 00:09:22|     MBC1 HD|     28138|[21.0362, 52.2394]|    SALIVE|      BADR_4&6|1709117223.6836503|2024-01-06T01:22:...|                  0|\n",
      "+--------------------+-------------------+------------+----------+------------------+----------+--------------+------------------+--------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Downstream Table:\n",
      "+--------------------+-------------------+------------+----------+\n",
      "|              mac_id|           datetime|channel_name|program_id|\n",
      "+--------------------+-------------------+------------+----------+\n",
      "|4136b85db09a646f2...|2024-01-06 00:09:00|     MBC1 HD|     28138|\n",
      "+--------------------+-------------------+------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Error loading data into Redshift for file C:/Users/vrnp2/MBC\\events_20240106.json: name 'redshift_url' is not defined\n",
      "Processing file: C:/Users/vrnp2/MBC\\events_20240107.json\n",
      "Dynamic key found in the JSON file: events_20240107\n",
      "Raw Data Table:\n",
      "+--------------------+-------------------+------------+----------+------------------+----------+--------------+------------------+--------------------+-------------------+\n",
      "|              mac_id|           datetime|channel_name|program_id|      geo_location|event_code|satellite_name|         timestamp|          index_time|duration_in_seconds|\n",
      "+--------------------+-------------------+------------+----------+------------------+----------+--------------+------------------+--------------------+-------------------+\n",
      "|4136b85db09a646f2...|2024-01-07 00:09:22|     MBC1 HD|     27943|[21.0362, 52.2394]|    SALIVE|      BADR_4&6|1709117223.6836503|2024-01-07T00:12:...|                  0|\n",
      "+--------------------+-------------------+------------+----------+------------------+----------+--------------+------------------+--------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Downstream Table:\n",
      "+--------------------+-------------------+------------+----------+\n",
      "|              mac_id|           datetime|channel_name|program_id|\n",
      "+--------------------+-------------------+------------+----------+\n",
      "|4136b85db09a646f2...|2024-01-07 00:09:00|     MBC1 HD|     27943|\n",
      "+--------------------+-------------------+------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Error loading data into Redshift for file C:/Users/vrnp2/MBC\\events_20240107.json: name 'redshift_url' is not defined\n"
     ]
    }
   ],
   "source": [
    "for file_path in raw_logs_files:\n",
    "    try:\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "\n",
    "        # Read the JSON file\n",
    "        raw_logs_df = spark.read.option(\"multiline\", \"true\").json(file_path)\n",
    "\n",
    "        # Get the first top-level key dynamically\n",
    "        dynamic_key = raw_logs_df.columns[0]\n",
    "        print(f\"Dynamic key found in the JSON file: {dynamic_key}\")\n",
    "\n",
    "        # Call the transformation function which iclude all the transformation(cleaning, validating ,enriching)\n",
    "        transformed_data = process_and_transform_json(raw_logs_df, program_logs_df)\n",
    "\n",
    "        # print(\"Transformed and Deduplicated DataFrame:\")\n",
    "        # transformed_data.show(1)\n",
    "\n",
    "        # Raw Data Table: Selecting all the columns for auditing and troubleshooting purposes\n",
    "        raw_data_df = transformed_data.select(\n",
    "            col(\"mac_id\"),\n",
    "            col(\"datetime\"),\n",
    "            col(\"channel_name\"),\n",
    "            col(\"program_id\"),\n",
    "            col(\"geo_location\"),\n",
    "            col(\"event_code\"),\n",
    "            col(\"satellite_name\"),\n",
    "            col(\"timestamp\"),\n",
    "            col(\"index_time\"),\n",
    "            col(\"duration_in_seconds\")\n",
    "        )\n",
    "\n",
    "        # Downstream Table\n",
    "        downstream_df = transformed_data.withColumn(\n",
    "            \"minute_level\", \n",
    "            to_timestamp(concat_ws(\" \", col(\"datetime\").cast(\"date\"), col(\"datetime\").cast(\"timestamp\").substr(12, 5)))\n",
    "        )\n",
    "        \n",
    "        # Deduplicate by mac_id and minute_level, keeping only the latest record within the same minute\n",
    "        window_spec = Window.partitionBy(\"mac_id\", \"minute_level\").orderBy(col(\"datetime\").desc())\n",
    "        downstream_df = downstream_df.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "            .filter(col(\"row_num\") == 1) \\\n",
    "            .select(\n",
    "                col(\"mac_id\"),\n",
    "                col(\"minute_level\").alias(\"datetime\"),\n",
    "                col(\"channel_name\"),\n",
    "                col(\"program_id\")\n",
    "            )\n",
    "        \n",
    "        print(\"Raw Data Table:\")\n",
    "        raw_data_df.show(1)\n",
    "        \n",
    "        print(\"Downstream Table:\")\n",
    "        downstream_df.show(1)\n",
    "\n",
    "\n",
    "        # Try-Catch Block for Redshift Loading\n",
    "        try:\n",
    "            # Load Raw Data Table\n",
    "            raw_data_df.write \\\n",
    "                .format(\"jdbc\") \\\n",
    "                .option(\"url\", redshift_url) \\\n",
    "                .option(\"dbtable\", \"raw_data_table\") \\\n",
    "                .option(\"user\", redshift_user_name) \\\n",
    "                .option(\"password\", redshift_password) \\\n",
    "                .option(\"driver\", \"com.amazon.redshift.jdbc42.Driver\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .save()\n",
    "            \n",
    "            # Load Downstream Table\n",
    "            downstream_df.write \\\n",
    "                .format(\"jdbc\") \\\n",
    "                .option(\"url\", redshift_url) \\\n",
    "                .option(\"dbtable\", \"downstream_table\") \\\n",
    "                .option(\"user\", redshift_user) \\\n",
    "                .option(\"password\", redshift_password) \\\n",
    "                .option(\"driver\", \"com.amazon.redshift.jdbc42.Driver\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .save()\n",
    "            print(f\"Successfully processed and saved file: {file_path}\")\n",
    "\n",
    "        except Exception as redshift_error:\n",
    "            print(f\"Error loading data into Redshift for file {file_path}: {redshift_error}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffdb85c-d194-4f51-9db8-4f3b5894b6e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
